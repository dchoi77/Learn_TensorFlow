{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.tensorflow.org/\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "# Tensors\n",
    "\n",
    "## Basic functions/attributes\n",
    "\n",
    "### tf.constant(), tf.ragged.constant(), tf.string()\n",
    "\n",
    "A tf.Tensor is immutable. You can't change a tensor once it's created. It has a value, but no state.\n",
    "\n",
    "A tensor with variable numbers of elements along some axis is called \"ragged\".\n",
    "\n",
    "```python\n",
    "t = tf.ragged.constant([[0,1,2,3],[4,5],[6,7,8],[9]])\n",
    "t.shape  \n",
    "# TensorShape([4, None])\n",
    "```\n",
    "\n",
    "### tf.Variable()\n",
    "\n",
    "```python\n",
    "a = tf.Variable([2.0, 3.0])\n",
    "a.assign([1, 2])            # it doesn't change the dtype; a has [1., 2.]\n",
    "\n",
    "a.assign_add([2,3])         # a has [3., 5.]\n",
    "a.assign_sub([7,9])         # a has [-4., -4.]\n",
    "\n",
    "b = a + 1         # a variable tensor + a tensor is a tensor, not a variable.\n",
    "\n",
    "b = tf.Variable([2.0, 3.0], trainable=False)     # not need gradients\n",
    "```\n",
    "\n",
    "### tf.convert_to_tensor(), tf.cast()\n",
    "\n",
    "### tf.newaxis, tf.reshape(), tf.broadcast_to()\n",
    "\n",
    "### tf.add(), tf.multiply(), tf.matmul(), tf.square(), tf.reduce_mean()\n",
    "\n",
    "### tf.linspace()\n",
    "\n",
    "### tf.random.normal()\n",
    "\n",
    "### tf.device()\n",
    "\n",
    "```python\n",
    "with tf.device('CPU:0'):\n",
    "    # ...\n",
    "with tf.device('GPU:0'):\n",
    "    # ...\n",
    "```\n",
    "\n",
    "### tf.strings\n",
    "\n",
    "* split(), bytes_split(), unicode_split()\n",
    "* unicode_decode(), unicode_encode()\n",
    "* to_number()\n",
    "\n",
    "\n",
    "### tf.GradientTape()\n",
    "\n",
    "Use tf.GradientTape() to train and/or compute gradients in eager.\n",
    "\n",
    "### persistent\n",
    "\n",
    "GradientTape.gradient can only be called once on non-persistent tapes.\n",
    "\n",
    "```python\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "tape.gradient(y, x)    # 2*x which is 6.0    \n",
    "tape.gradient(z, y)    # 2*y which is 2*x*x = 18.0\n",
    "tape.gradient(z, x)    # 108.0 which is 6.0*18.0\n",
    "```\n",
    "\n",
    "#### watch()\n",
    "\n",
    "tf.Variable (with trainable=True) is watched by default, but tf.Tensor is not watched by default.\n",
    "\n",
    "```python\n",
    "x0 = tf.Variable(3.0, name='x0')                   # trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)  # not trainable\n",
    "x2 = x0 + 1.0                                      # not trainable, since it's a tensor\n",
    "x3 = tf.constant(3.0, name='x3')                   # not trainable\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x0) + tf.square(x1) + tf.square(x2) + tf.square(x3)\n",
    "grad = tape.gradient(y, [x0,x1,x2,x3])\n",
    "\n",
    "grad\n",
    "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None, None, None]\n",
    "\n",
    "tape.watched_variables()\n",
    "(<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>,)\n",
    "```\n",
    "\n",
    "Use watch():\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([x1,x2,x3])\n",
    "    y = tf.square(x0) + tf.square(x1) + tf.square(x2) + tf.square(x3)\n",
    "grad = tape.gradient(y, [x0,x1,x2,x3])\n",
    "\n",
    "grad\n",
    "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
    " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
    " <tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n",
    " <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n",
    "\n",
    "tape.watched_variables()\n",
    "(<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>,\n",
    " <tf.Variable 'x1:0' shape=() dtype=float32, numpy=3.0>)\n",
    "```\n",
    "\n",
    "#### watch_accessed_variables\n",
    "To disable the default behavior of watching all tf.Variables, set `watch_accessed_variables=False`:\n",
    "\n",
    "```python\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)          # Only x1 is watched.\n",
    "    # ...\n",
    "```\n",
    "\n",
    "#### non-scalar targets\n",
    "\n",
    "A gradient tape will compute \n",
    "\n",
    "* $\\nabla_{\\mathbf{x}} y$, if $y$ is a scalar function of $\\mathbf{x}=(x_1,\\ldots,x_n)$,\n",
    "\n",
    "* $\\nabla_{\\mathbf{x}}  \\sum_i y_i$, if $y_i$ is scalar function of $\\mathbf{x}=(x_1,\\ldots,x_n)$.\n",
    "\n",
    "```python\n",
    "x = tf.Variable([2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "tape.gradient([y0, y1], x)\n",
    "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.75    , 5.888889], dtype=float32)>\n",
    "\n",
    "# The above is same as:\n",
    "x = tf.Variable([2.0, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "tape.gradient(y0, x) + tape.gradient(y1, x)\n",
    "```\n",
    "        \n",
    "Similarly, if the target is not scalar, the gradient of the sum is calculated:\n",
    "\n",
    "```python\n",
    "x = tf.Variable([[2.0, 3.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.transpose(x) @ x       # 2-by-2           \n",
    "tape.gradient(y, x)              # shape: (1,2), since x.shape is (1,2)\n",
    "\n",
    "# The above is same as:  \n",
    "x = tf.Variable([[2.0, 3.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.transpose(x) @ x       # 2-by-2\n",
    "    y1, y2, y3, y4 = (y[i][j] for i in [0,1] for j in [0,1])\n",
    "tape.gradient([y1,y2,y3,y4], x)\n",
    "```\n",
    "\n",
    "#### A gradient of None\n",
    "\n",
    "```python\n",
    "# Ex1\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y * y\n",
    "tape.gradient(z, x)  # None\n",
    "\n",
    "# Ex2\n",
    "x = tf.Variable(2.0)\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x+1\n",
    "    x = x + 1        # After this operation, x is now a Tensor, not a Variable; it should be x.assign_add(1).\n",
    "\n",
    "# Ex3    \n",
    "with tf.GradientTape() as tape:\n",
    "    y = np.mean(x2, axis=0)\n",
    "    y = tf.reduce_mean(y, axis=0)  # y is a constant tensor\n",
    "    \n",
    "# Ex4\n",
    "x = tf.Variable([[2, 2], [2, 2]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.reduce_sum(x)           # x.dtype should not be int.\n",
    "print(tape.gradient(y, x))\n",
    "```\n",
    "\n",
    "### tf.sparse\n",
    "\n",
    "* SparseTensor()\n",
    "\n",
    "### tf.io\n",
    "\n",
    "* decode_raw()\n",
    "\n",
    "```python\n",
    "tf.io.decode_raw(\"Duck\", tf.uint8)\n",
    "<tf.Tensor: shape=(4,), dtype=uint8, numpy=array([ 68, 117,  99, 107], dtype=uint8)>\n",
    "```\n",
    "\n",
    "### tf.nn\n",
    "\n",
    "* tf.nn.softmax()\n",
    "* tf.nn.softplus(): log(exp(x) + 1)\n",
    "* tf.nn.sigmoid(): 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "## Example of training a model\n",
    "\n",
    "```python\n",
    "# Data\n",
    "shuffle_size, batch_size = 1000, 32\n",
    "input_shape = (28,28,1)\n",
    "\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.cast(mnist_images[..., tf.newaxis]/255, tf.float32),\n",
    "     tf.cast(mnist_labels, tf.int64))\n",
    ").shuffle(shuffle_size).batch(batch_size)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,[3,3],activation='relu',input_shape=input_shape),\n",
    "    tf.keras.layers.Conv2D(16,[3,3],activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)])\n",
    "                          \n",
    "# Optimizer/Loss\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                          \n",
    "# Train a model\n",
    "def train_step(batch, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch, training=True)\n",
    "        loss = loss_fn(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss.numpy().mean()\n",
    "\n",
    "epochs = 1\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    for batch, labels in dataset:\n",
    "        batch_loss_mean = train_step(batch, labels)\n",
    "        loss_history.append(batch_loss_mean)\n",
    "    print('Epoch {}, Loss {}'.format(epoch+1, np.mean(loss_history)))\n",
    "    \n",
    "model.save_weights('weights')\n",
    "status = model.load_weights('weights')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
