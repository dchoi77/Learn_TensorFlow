{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://www.tensorflow.org/\n",
    "\n",
    "\n",
    "# Image classification\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "```python\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "```\n",
    "\n",
    "* train_images: (60000,28,28); entries are integers ranging from 0 to 255\n",
    "* train_labels: (60000,); entries are integers ranging from 0 to 9\n",
    "\n",
    "```python\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "```\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "input_shape = train_images[0].shape\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.Dense(28, activation='relu'),\n",
    "    keras.layers.Dense(10)])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "print(test_labels[0], np.argmax(predictions[0]))\n",
    "```\n",
    "\n",
    "We may use a probability model using `Softmax()`:\n",
    "\n",
    "```python\n",
    "probability_model = keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_images)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "## with TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "```python\n",
    "!pip install tensorflow-hub\n",
    "!pip install tfds-nightly\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data, validation_data, test_data = tfds.load(name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)\n",
    "```\n",
    "\n",
    "There are 15,000 examples for training, 10,000 examples for validation and 25,000 examples for testing.\n",
    "\n",
    "\n",
    "Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way.\n",
    "\n",
    "```python\n",
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "```\n",
    "\n",
    "* `train_examples_batch` is a tf.Tensor of shape=(10,) and dtype=string.\n",
    "* `train_labels_batch` is a tf.Tensor of shape=(10,) and dtype=int64 (0 or 1).\n",
    "\n",
    "__Model__:\n",
    "\n",
    "* We use a pre-trained text embedding model from TensorFlow Hub as the first layer of our model.\n",
    "* `hub_layer(train_examples_batch)` outputs a tf.Tensor of shape=(10,20) and dtype=float32.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential()\n",
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "__Train the model__:\n",
    "\n",
    "```python\n",
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=1)\n",
    "```\n",
    "* Since the number of training examples is 15000 and batch_size is 512, each epoch iterates 30 times (15000/512 = 29.xxx)\n",
    "\n",
    "__Evaluate the model__:\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_data.batch(512), verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with preprocessed text\n",
    "\n",
    "__Dataset__: IDMB movie review\n",
    "\n",
    "* The dataset (`train_data` and `test_data`) comes preprocessed.\n",
    "    * Each example is an array of integers greater than 0.\n",
    "    * Each integer represents a specific word-piece in the dictionary.\n",
    "    * The lengths of examples are not the same.\n",
    "    \n",
    "* Each label is an integer value of either 0 or 1.\n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train_data.shuffle(BUFFER_SIZE).padded_batch(32)\n",
    "test_batches = test_data.padded_batch(32)\n",
    "```\n",
    "* Use padded_batch() to zero pad the examples.\n",
    "* Each batch will have a shape of (`batch_size`, `sequence_length`)\n",
    "* `sequence_length` depends on each batch. For example, the first batch has a shape of (32, 1352) and the second batch has a shape of (32, 777).\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "VOCAB_SIZE = 8185\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(VOCAB_SIZE, 16),\n",
    "  keras.layers.GlobalAveragePooling1D(),\n",
    "  keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_batches,\n",
    "                    validation_steps=30)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_batches)\n",
    "```\n",
    "\n",
    "* `Embedding()` turns positive integers (indexes) into dense vectors of fixed size. \n",
    "* Suppose that X is a batch whose shape is (batch_size, sequence_length). If X\\[i,j\\] is an integer n (0 <= n < VOCAB_SIZE), then the embedding layer turns n into a vector of length 16 in this example. \n",
    "* Thus the output of the embedding layer will have a shape of (batch_size, sequence_length, 16). \n",
    "\n",
    "\n",
    "* `GlobalAveragePooling1D(data_format='channels_last')` turns a 3D tensor with shape `(batch_size, steps, features)` into \n",
    " a 2D tensor with shape `(batch_size, features)`.\n",
    "* After the embedding layer, we may assume that each token of a review is expressed as a dense vector of size 16. `GlobalAveragePooling1D` computes the average of the dense vectors for each review.\n",
    "* We may assume that `GlobalAveragePooling1D` expresses each review as a vector of length 16.\n",
    "* Thus after the `GlobalAveragePooling1D` layer, the shape of a batch will be `(batch_size, 16)`.\n",
    "\n",
    "\n",
    "* `validation_steps` in `model.fit()`: If `validation_steps` is None, validation will run until the `validation_data` dataset is exhausted. In the case of a infinite dataset, it will run into a infinite loop. If `validation_steps` is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "```python\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t', sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "train_means, train_stds = train_dataset.mean(0), train_dataset.std(0)\n",
    "normed_train_data = (train_data - train_means) / train_stds\n",
    "normed_test_data = (test_data - train_means) / train_stds\n",
    "```\n",
    "\n",
    "__Model__:\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(train_dataset.shape[1],)),\n",
    "    layers.Dense(64, activation='rely'),\n",
    "    layers.Dense(1)])\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.RMSprop(0.001), metrics=['mae','mse'])\n",
    "\n",
    "EPOCHS = 1000\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "history = model.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split=0.2, verbose=0, callbacks=[early_stop])\n",
    "\n",
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
