{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with an RNN\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "* The IMDB large movie review dataset is a binary classification dataset.\n",
    "\n",
    "```python\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "train_dataset = dataset['train'].shuffle(10000).padded_batch(BATCH_SIZE)\n",
    "test_dataset = dataset['test'].padded_batch(BATCH_SIZE)\n",
    "```\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "* Shapes:\n",
    "    * inputs: (batch_size, seq_length)\n",
    "    * after Embedding(): (batch_size, seq_length, 64)\n",
    "    * after Birectional(): (batch_size, 128)\n",
    "    * after Dense(): (batch_size, 64)\n",
    "    * after Dense(): (batch_size, 1)\n",
    "    \n",
    "```python\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "```\n",
    "\n",
    "* The output of `model.predict()` is not necessarily between 0 and 1, since the final dense layer of the model does not use an activation function such as `tanh`.\n",
    "\n",
    "* If `model.predict()` > 0, then the predicted label is 1. Otherwise, 0.\n",
    "\n",
    "__Model__ (using two LSTM layers):\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "* Shapes:\n",
    "    * inputs: (batch_size, seq_length)\n",
    "    * after Embedding: (batch_size, seq_length, 64)\n",
    "    * after Bidirectional: (batch_size, seq_length, 128)\n",
    "    * after Bidrectional: (batch_size, 64)\n",
    "    * after Dense: (batch_size, 64)\n",
    "    * after Dense: (batch_size, 1)\n",
    "    \n",
    "* The first LSTM layer uses `return_sequences=True` so that its output preserves the axis of timesteps and the second LSTM layer can be used.\n",
    "\n",
    "* We can stack multiple RNN layers in such a way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with an RNN\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "```python\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')    # str\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)     # 65\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])             # entries are integers ranging from 0 to 64.\n",
    "\n",
    "\n",
    "seq_length = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "dataset = dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = dataset.map(lambda chunk: (chunk[:-1], chunk[1:]))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "```\n",
    "\n",
    "* The shape of each batch is ((64, 100), (64,100)).\n",
    "* Entries of a batch are integers ranging from 0 to vocab_size-1.\n",
    "* Note that we did not use `dataset.shuffle()` before batching the dataset.\n",
    "\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback], shuffle=False)\n",
    "```\n",
    "\n",
    "\n",
    "* Shapes through the model:\n",
    "    * inputs: (batch_size, x)\n",
    "    * after Embedding: (batch_size, x, embedding_dim)\n",
    "    * after GRU: (batch_size, x, rnn_units)\n",
    "    * after Dense: (batch_size, x, vocab_size)\n",
    "    * Here x is set to `seq_length` during training, but it can be set to any number for a test.\n",
    "* Roughly, if input is a string of length m, then model(input) returns a string of the same length.\n",
    "\n",
    "* Note that `batch_input_shape` is set as `[batch_size, None]`. Moreover, `drop_remainder=True` was used in building the dataset.\n",
    "\n",
    "* `batch_size` is fixed, but `seq_length` can be variable. We can rebuild the trained model later and set a different value for `batch_size`.\n",
    "\n",
    "* Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "\n",
    "* `model.layers[1].states[0].shape` is (batch_size, rnn_units).\n",
    "\n",
    "* Note also `stateful=True` in `GRU`. The following is from the source of recurrent.py:\n",
    "\n",
    "Note on using statefulness in RNNs:\n",
    "    You can set RNN layers to be 'stateful', which means that the states\n",
    "    computed for the samples in one batch will be reused as initial states\n",
    "    for the samples in the next batch. This assumes a one-to-one mapping\n",
    "    between samples in different successive batches.\n",
    "    To enable statefulness:\n",
    "      - Specify `stateful=True` in the layer constructor.\n",
    "      - Specify a fixed batch size for your model, by passing\n",
    "        If sequential model:\n",
    "          `batch_input_shape=(...)` to the first layer in your model.\n",
    "        Else for functional model with 1 or more Input layers:\n",
    "          `batch_shape=(...)` to all the first layers in your model.\n",
    "        This is the expected shape of your inputs\n",
    "        *including the batch size*.\n",
    "        It should be a tuple of integers, e.g. `(32, 10, 100)`.\n",
    "      - Specify `shuffle=False` when calling fit().\n",
    "    To reset the states of your model, call `.reset_states()` on either\n",
    "    a specific layer, or on your entire model.\n",
    "    \n",
    "\n",
    "\n",
    "__Rebuil the model__ (batch_size=1):\n",
    "\n",
    "```python\n",
    "model1 = build_model(vocab_size, embedding_dim, rnn_units, 1)\n",
    "model1.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model1.build(tf.TensorShape([1,None]))\n",
    "```\n",
    "\n",
    "__Generate text__:\n",
    "\n",
    "```python\n",
    "start_string = u\"ROMEO: \"\n",
    "num_generate = 1000\n",
    "\n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "text_generated = []\n",
    "model1.reset_states()\n",
    "for i in range(num_generate):\n",
    "    predictions = model1(input_eval)                                 # (1, len(input_eval), vocab_size)\n",
    "    predictions = tf.squeeze(predictions, 0)                         # (len(input_eval), vocab_size)\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "result = start_string + ''.join(text_generated)\n",
    "```\n",
    "\n",
    "* When i=0, the state of the RNN layer is updated by using start_string.\n",
    "* When i>0, input_eval is a tensor of length 1.\n",
    "* The state of the RNN layer is updated at each iteration.\n",
    "\n",
    "\n",
    "__Customized training__:\n",
    "\n",
    "```python\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTage() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    hidden = model.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, taget)\n",
    "        if batch_n % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {}'.format(epoch+1, batch_n, loss))\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "```\n",
    "\n",
    "* `sparse_categorical_crossentropy()` returns a tensor of shape (batch_size, seq_length).\n",
    "* `tf.reduce_mean(x)` is a tensor having the value `x.numpy().flatten().mean()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with Attention\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "Language translation from Spanish to English.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "```python\n",
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip',\n",
    "                                      origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "                                      extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "```\n",
    "\n",
    "* The format of each line is 'eng_expression\\tspa_expression'. For example, `lines[0]` is 'Go.\\tVe.'.\n",
    "\n",
    "```python\n",
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip() \n",
    "    w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn') \n",
    "    w = re.sub(r\"[^\\w?.!,¿]+\", \" \", w)\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'\\s+', \" \", w).strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "num_examples = None\n",
    "\n",
    "word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "en_texts, sp_texts = zip(*word_pairs)\n",
    "```\n",
    "\n",
    "* `NFD` stands for Normalization Form Canonical Decomposition.\n",
    "* `Mn` stands for Markdown, nonspacing.\n",
    "* `en` and `sp` are tuples of strings. \n",
    "* `en[0]` is '&lt;start&gt; go . &lt;end&gt;' and `sp[0]` is '&lt;start&gt; ve . &lt;end&gt;'.\n",
    "\n",
    "```python\n",
    "def tokenize(texts):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)  # a list of lists of positive integers \n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post') # a 2d npumpy array\n",
    "    return sequences, tokenizer\n",
    "\n",
    "inp_sequences, inp_tokenizer = tokenize(sp_texts)\n",
    "targ_sequences, targ_tokenizer = tokenize(en_texts)\n",
    "\n",
    "inp_train, inp_val, targ_train, targ_val = train_test_split(inp_sequences, targ_sequences, test_size=0.2)\n",
    "\n",
    "```\n",
    "* `inp_sequences` is a numpy array of shape (118964, 53) consisting of integers.\n",
    "* `targ_sequences` ia a numpy array of shape (118964, 51) consisting of integers.\n",
    "\n",
    "```python\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp_train, targ_train))\\\n",
    ".shuffle(BUFFER_SIZE)\\\n",
    ".batch(BATCH_SIZE, drop_remainder=True)\n",
    "```\n",
    "\n",
    "__Model__: \n",
    "\n",
    "See `Learn_Models/Models_ANNs.ipynb` for more information on the encoder-decoder model with attention.\n",
    "\n",
    "```python\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        score = self.V(tf.nn.tanh(self.W1(tf.expand_dims(query,1)) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sze = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        output = self.fc(output)\n",
    "        return output, state, attention_weights  \n",
    "```\n",
    "\n",
    "```python\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_tokenizer.word_index)+1\n",
    "vocab_targ_size = len(targ_tokenizer.word_index)+1\n",
    "seq_length_inp = inp_sequences.shape[1]\n",
    "seq_length_targ = targ_sequences.shape[1]\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)\n",
    "```\n",
    "\n",
    "__Optimizer and Loss function__:\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ *= tf.cast(mask, dtype=loss_.dtype)\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "```\n",
    "\n",
    "* `reduction`: Default value is `AUTO`. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`.\n",
    "\n",
    "* We set `reduction` to `none`, since the target vector was padded with 0s so that we need to mask the padding before we compute the loss.\n",
    "\n",
    "\n",
    "__Training__:\n",
    "\n",
    "```python\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:,t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:,t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "steps_per_epoch = len(inp_train)//BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch_index, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch_index, batch_loss.numpy()))\n",
    "            \n",
    "    if (epoch+1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "```\n",
    "\n",
    "* At each epoch, `enc_hidden` is set to `encoder.initialize_hidden_state()` which is simply a zero tensor of shape (batch_size, endec_units). The tensor is not updated at each epoch and each batch iteration. That is, in `train_step(inp, targ, enc_hidden)`, `enc_hidden` is always a zero tensor. We may place `enc_hidden = encoder.initialize_hidden_state()` before `for epoch in range(EPOCHS):`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "* `img_name_vector` is a list of image path names. For example, \n",
    "```\n",
    "img_name_vector[0]\n",
    ": '/content/train2014/COCO_train2014_000000324909.jpg'\n",
    "```\n",
    "* `train_captions` is a list of image captions. For example,\n",
    "```\n",
    "train_captions[0]\n",
    ": '<start> A skateboarder performing a trick on a skateboard ramp. <end>'\n",
    "```\n",
    "\n",
    "\n",
    "__Model extracting features__:\n",
    "\n",
    "```python\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "image_features_extract_model = tf.keras.Model(image_model.input, image_model.layers[-1].output)\n",
    "```\n",
    "* input.shape: (batch_size, 299, 299, 3)\n",
    "* output.shape: (batch_size, 8, 8, 2048)\n",
    "\n",
    "\n",
    "__Cache image features to disk__:\n",
    "\n",
    "```python\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299,299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(sorted(set(img_name_vector)))\\  # some image names are the same\n",
    "    .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .batch(16)\n",
    "\n",
    "for batch_img, batch_path in image_dataset:\n",
    "    batch_features = image_features_extract_model(batch_img)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    for bf, p in zip(batch_features, batch_path):\n",
    "        np.save(p.numpy().decode(\"utf-8\"), bf.numpy())\n",
    "```\n",
    "\n",
    "__Preprocess and tokenize captions__:\n",
    "\n",
    "```python\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \n",
    "                                                  oov_token=\"<unk>\", \n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')  #$\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "max_length = max(len(t) for t in train_seqs)                # Every sample has the same sequence length.\n",
    "\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "```\n",
    "\n",
    "__Split the data into training and testing__:\n",
    "\n",
    "```python\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)\n",
    "```                                                                    \n",
    "\n",
    "__Create a tf.data dataset for training__:\n",
    "\n",
    "```python\n",
    "def map_func(img_name, cap):\n",
    "    return np.load(img_name.decode('utf-8')+'.npy'), cap\n",
    "    \n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\\\n",
    "    .map(lambda img_name, cap: tf.numpy_function(map_func, [img_name, cap], [tf.float32, tf.int32]), \n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "    .shuffle(BUFFER_SIZE)\\\n",
    "    .batch(BATCH_SIZE)\\\n",
    "    .prebatch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "```\n",
    "\n",
    "* Each batch has shape ((BATCH_SIZE, 64, 2048), (BATCH_SIZE, max_length))\n",
    "\n",
    "\n",
    "__Model__:\n",
    "\n",
    "* Encoder:\n",
    "    * inputs: (batch_size, 64, 2048); 64 features; each feature is a 2048-dimensional vector. \n",
    "    * outputs: (batch_size, 64, 256); each feature is encoded to a 256-dimensional vector.\n",
    "    * 64 and 256 are like `enc_seq_length` and `enc_units` in the encoder used Translation with Attention.\n",
    "    * It does not have an embedding layer or a GRU layer.\n",
    "    \n",
    "```python\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.nn.relu(self.fc(x))\n",
    "```\n",
    "    \n",
    "* Decoder:\n",
    "    * The decoder is similar to the one used in Translation with Attention.\n",
    "    * inputs: (batch_size, 1)\n",
    "    * outputs: (batch_size, vocab_size)\n",
    "    \n",
    "* In Translation with Attention, enc_units is equal to dec_units. But in this example, enc_units is 256 (embedding_dim) and dec_units is 512 (units).\n",
    "\n",
    "* In Translation with Attention, the hidden state of the decoder is initially set by the hidden state of the encoder at its last timestep. The hidden state of the decoder in this example, however, is simply set to a zero tensor of shape (batch_size, units). In the language translation, it is meaningful to use the hidden state of the encoder at the last timestep. But we deal with images in this example. What is the data belong to the last timestep in the encoder? There are 64 features in the encoder side, but the features are not in time order.\n",
    "\n",
    "```python\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "    \n",
    "    def call(self, x, features, hidden):\n",
    "        # x.shape: (batch_size, 1)\n",
    "        # features.shape: (batch_size, 64, embedding_dim)\n",
    "        # hidden.shape: (batch_size, units)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, features)\n",
    "        x = self.embedding(x)                                              # x: (batch_size, 1, embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)     # x: (batch_size, 1, embedding_dim + units)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc1(output)                       # (batch_size, 1, units)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))        # (batch_size, units)\n",
    "        x = self.fc2(x)                            # (batch_size, vocab_size)\n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n",
    "```\n",
    "\n",
    "__Training__:\n",
    "\n",
    "```python\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    return tf.reduce_mean(loss_ * tf.cast(mask, dtype=loss_.dtype))\n",
    "    \n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "                           \n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "```\n",
    "\n",
    "```python\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:,i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:,i], 1)\n",
    "            \n",
    "    batch_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return batch_loss\n",
    "    \n",
    "\n",
    "num_steps = len(img_name_train) // BATCH_SIZE    \n",
    "EPOCHS = 20\n",
    "loss_plot = []\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    for (batch_no, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss = train_step(img_tensor, target)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch_no % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch_no, batch_loss))\n",
    "\n",
    "    sofar_loss = total_loss / num_steps\n",
    "    loss_plot.append(sofar_loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1, sofar_loss))\n",
    "```\n",
    "\n",
    "__Captioning__:\n",
    "\n",
    "```python\n",
    "def evaluate(image):\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)           # shape: (1, 8, 8, 2048)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "                                                                        # shape: (1, 64, 2048)\n",
    "    features = encoder(img_tensor_val)\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "            \n",
    "    return result\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
