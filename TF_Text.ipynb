{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Text classification with an RNN</font>\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "* The IMDB large movie review dataset is a binary classification dataset.\n",
    "\n",
    "```python\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "train_dataset = dataset['train'].shuffle(10000).padded_batch(BATCH_SIZE)\n",
    "test_dataset = dataset['test'].padded_batch(BATCH_SIZE)\n",
    "```\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "* Shapes:\n",
    "    * inputs: (batch_size, seq_length)\n",
    "    * after Embedding(): (batch_size, seq_length, 64)\n",
    "    * after Birectional(): (batch_size, 128)\n",
    "    * after Dense(): (batch_size, 64)\n",
    "    * after Dense(): (batch_size, 1)\n",
    "    \n",
    "```python\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset, validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "```\n",
    "\n",
    "* The output of `model.predict()` is not necessarily between 0 and 1, since the final dense layer of the model does not use an activation function such as `tanh`.\n",
    "\n",
    "* If `model.predict()` > 0, then the predicted label is 1. Otherwise, 0.\n",
    "\n",
    "__Model__ (using two LSTM layers):\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "* Shapes:\n",
    "    * inputs: (batch_size, seq_length)\n",
    "    * after Embedding: (batch_size, seq_length, 64)\n",
    "    * after Bidirectional: (batch_size, seq_length, 128)\n",
    "    * after Bidrectional: (batch_size, 64)\n",
    "    * after Dense: (batch_size, 64)\n",
    "    * after Dense: (batch_size, 1)\n",
    "    \n",
    "* The first LSTM layer uses `return_sequences=True` so that its output preserves the axis of timesteps and the second LSTM layer can be used.\n",
    "\n",
    "* We can stack multiple RNN layers in such a way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Text generation with an RNN</font>\n",
    "\n",
    "References: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "__Dataset__:\n",
    "\n",
    "```python\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')    # str\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)     # 65\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])             # entries are integers ranging from 0 to 64.\n",
    "\n",
    "\n",
    "seq_length = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "dataset = dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = dataset.map(lambda chunk: (chunk[:-1], chunk[1:]))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "```\n",
    "\n",
    "* The shape of each batch is ((64, 100), (64,100)).\n",
    "* Entries of a batch are integers ranging from 0 to vocab_size-1.\n",
    "* Note that we did not use `dataset.shuffle()` before batching the dataset.\n",
    "\n",
    "\n",
    "__Model__:\n",
    "\n",
    "```python\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback], shuffle=False)\n",
    "```\n",
    "\n",
    "\n",
    "* Shapes through the model:\n",
    "    * inputs: (batch_size, x)\n",
    "    * after Embedding: (batch_size, x, embedding_dim)\n",
    "    * after GRU: (batch_size, x, rnn_units)\n",
    "    * after Dense: (batch_size, x, vocab_size)\n",
    "    * Here x is set to `seq_length` during training, but it can be set to any number for a test.\n",
    "* Roughly, if input is a string of length m, then model(input) returns a string of the same length.\n",
    "\n",
    "* Note that `batch_input_shape` is set as `[batch_size, None]`. Moreover, `drop_remainder=True` was used in building the dataset. \n",
    "* Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
    "* `model.layers[1].states[0].shape` is (batch_size, rnn_units).\n",
    "* Note also `stateful=True` in `GRU`. The following is from the source of recurrent.py:\n",
    "\n",
    "Note on using statefulness in RNNs:\n",
    "    You can set RNN layers to be 'stateful', which means that the states\n",
    "    computed for the samples in one batch will be reused as initial states\n",
    "    for the samples in the next batch. This assumes a one-to-one mapping\n",
    "    between samples in different successive batches.\n",
    "    To enable statefulness:\n",
    "      - Specify `stateful=True` in the layer constructor.\n",
    "      - Specify a fixed batch size for your model, by passing\n",
    "        If sequential model:\n",
    "          `batch_input_shape=(...)` to the first layer in your model.\n",
    "        Else for functional model with 1 or more Input layers:\n",
    "          `batch_shape=(...)` to all the first layers in your model.\n",
    "        This is the expected shape of your inputs\n",
    "        *including the batch size*.\n",
    "        It should be a tuple of integers, e.g. `(32, 10, 100)`.\n",
    "      - Specify `shuffle=False` when calling fit().\n",
    "    To reset the states of your model, call `.reset_states()` on either\n",
    "    a specific layer, or on your entire model.\n",
    "    \n",
    "\n",
    "\n",
    "__Rebuil the model__ (batch_size=1):\n",
    "\n",
    "```python\n",
    "model1 = build_model(vocab_size, embedding_dim, rnn_units, 1)\n",
    "model1.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model1.build(tf.TensorShape([1,None]))\n",
    "```\n",
    "\n",
    "__Generate text__:\n",
    "\n",
    "```python\n",
    "start_string = u\"ROMEO: \"\n",
    "num_generate = 1000\n",
    "\n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "text_generated = []\n",
    "model1.reset_states()\n",
    "for i in range(num_generate):\n",
    "    predictions = model1(input_eval)                                 # (1, len(input_eval), vocab_size)\n",
    "    predictions = tf.squeeze(predictions, 0)                         # (len(input_eval), vocab_size)\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "result = start_string + ''.join(text_generated)\n",
    "```\n",
    "\n",
    "* When i=0, the state of the RNN layer is updated by using start_string.\n",
    "* When i>0, input_eval is a tensor of length 1.\n",
    "* The state of the RNN layer is updated at each iteration.\n",
    "\n",
    "\n",
    "__Customized training__:\n",
    "\n",
    "```python\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTage() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    hidden = model.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, taget)\n",
    "        if batch_n % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {}'.format(epoch+1, batch_n, loss))\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "```\n",
    "\n",
    "* `sparse_categorical_crossentropy()` returns a tensor of shape (batch_size, seq_length).\n",
    "* `tf.reduce_mean(x)` is a tensor having the value `x.numpy().flatten().mean()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
